{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "**Gym environment: the Cliff problem**\n",
    "\n",
    "In this exercise, we are going to explain the Gym interface for RL problems, using the Cliff problem:\n",
    "\n",
    "![alt text](cliff_problem.png \"Title\")\n",
    "\n",
    "The purpose of this example is to show how you can implement a basic MDP (Markov Decision Process) environment using the Gym library, and to illustrate some key concepts that have been explained in class.\n",
    "\n",
    "First, we start by importing the libraries we are going to use. You should be familiar to all of them, except to Gymnasium (previously known as Gym), which is the *the facto* standard for RL environments in Python. You may check its [webpage](https://github.com/Farama-Foundation/Gymnasium) to get more information."
   ],
   "metadata": {
    "id": "V2VHPvekdsh-"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "JPalqhgardzE",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "5f856b38-4653-4cf8-8b30-0a6f6451759e"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "try:\n",
    "    import gymnasium.spaces as gs\n",
    "except:\n",
    "    !pip install gymnasium\n",
    "    import gymnasium.spaces as gs\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, we are going to create the Cliff class. The key methods that we are going to need are:\n",
    "* **reset**: it restarts the environment, and returns the initial state (S, by default).\n",
    "* **step**: this method takes an action, and implements all the needed actions to simulate an environment step. It returns the next state, the reward, and whether the episode has ended (i.e., we have either reached the target state G or fallen off the cliff).\n",
    "\n",
    "While both previous methods are always required to have a Gym environment (de facto standard for Reinforcement Learning in Python), the other methods included are specific for this problem:\n",
    "* **from_index** and **to_index** change between the state coordinates in the 2-D grid and a scalar index to identify the state.\n",
    "* **reward_done** computes the reward and whether the episode has ended,\n",
    "depending on the state.\n",
    "* **action_to_str** takes as input the integer identifying the action and\n",
    "returns a string for human readability."
   ],
   "metadata": {
    "id": "m_T8ittJeX7L"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class cliff(object):  # Create the Cliff class\n",
    "\n",
    "    def __init__(self, dims=(3, 3), seed=1234):\n",
    "        self.dims = dims  # Cliff dims (rows x cols)\n",
    "        self.observation_space = gs.Box(low=np.array([0, 0]), high=np.array([self.dims[0] - 1, self.dims[1] - 1]), dtype=int, seed=seed)  # The observations are the coordinates of each grid cell\n",
    "        self.action_space = gs.Discrete(4, seed=seed)  #  There are 4 possible actions: 0=up, 1=down, 2=left, 3=right\n",
    "        self.init_state = (self.dims[0] - 1, 0)  # Initial state, always the same for simplicity: lower left corner\n",
    "        self.target_state = (self.dims[0] - 1, self.dims[1] - 1)  # Target state: always the same for simplicity: lower right corner\n",
    "        self.cliff_location = [(self.dims[0] - 1, i) for i in range(1, self.dims[1] - 1)]  # Lower row (except for target and initial state)\n",
    "        self.state = None  # This is to store data later on\n",
    "\n",
    "    def reset(self, randomize=False):  # Call this method to reset the environment\n",
    "        if randomize:  # To use a random initial state\n",
    "            self.state = self.observation_space.sample()\n",
    "        else:\n",
    "            self.state = self.init_state  # Use a fixed initial state\n",
    "        return self.state, None  # The None is provided to keep consistency with Gymnasium, which outputs the state and info\n",
    "\n",
    "    def step(self, action):  # This method implements the environment transition\n",
    "\n",
    "        action = np.squeeze(action).astype(int).item()  # Prepare action (make it an integer just in case)\n",
    "        if action < 0 or action > 3:  # Check the action bounds\n",
    "            raise RuntimeError('Action out of bounds')\n",
    "        # Perform action to get next state (i.e., move the agent in the grid world)\n",
    "        if action == 0:\n",
    "            next_state = self.state + np.array([1, 0])  # Move down\n",
    "        elif action == 1:\n",
    "            next_state = self.state + np.array([-1, 0])  # Move up\n",
    "        elif action == 2:\n",
    "            next_state = self.state + np.array([0, -1])  # Move left\n",
    "        elif action == 3:\n",
    "            next_state = self.state + np.array([0, 1])  # Move right\n",
    "        # Set the action bounds correctly, as we may end up out of the grid\n",
    "        next_state = np.clip(next_state, np.zeros(2), np.array(self.dims) - 1).astype(int)\n",
    "        # Check reward and termination condition\n",
    "        reward, done = self.reward_done(next_state)\n",
    "        self.state = next_state  # Change the state in the class\n",
    "        terminated = truncated = done  # This is to keep consistency with Gymnasium\n",
    "        return next_state, reward, terminated, truncated, {}  # Return next state, reward and whether episode has ended\n",
    "\n",
    "    def from_index(self, index):  # Ancillary method: convert an index state to state coordinates\n",
    "        return np.unravel_index(index, self.dims)\n",
    "\n",
    "    def to_index(self, state):  # Ancillary method: convert state coordinates to index\n",
    "        return np.ravel_multi_index(state, self.dims)\n",
    "\n",
    "    def reward_done(self, state):\n",
    "      if tuple(state) in self.cliff_location:  # We fall off the cliff\n",
    "        reward = -100  # Highly penalizing reward: you die\n",
    "        done = True  # Episode is terminated\n",
    "      elif np.sum(np.abs(state - np.array(self.target_state))) < 1e-4:  # Final target found\n",
    "        reward = 0  # Final reward is 0\n",
    "        done = True  # Episode is terminated\n",
    "      else:  # We haven't neither reached the target nor fallen off the cliff\n",
    "        reward = -1  # Standard reward (takes another step)\n",
    "        done = False  # Episode not terminated yet\n",
    "      return reward, done\n",
    "\n",
    "    def action_to_str(self, action):\n",
    "      def ac2str(action):\n",
    "        if action == 0:\n",
    "          return 'down'\n",
    "        if action == 1:\n",
    "          return 'up'\n",
    "        if action == 2:\n",
    "          return 'left'\n",
    "        if action == 3:\n",
    "          return 'right'\n",
    "      if isinstance(action, list):\n",
    "        return [ac2str(a) for a in action]\n",
    "      else:\n",
    "        return ac2str(action)"
   ],
   "metadata": {
    "id": "YCHn0ci9rjLm"
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "The next thing we do is to seed the random number generator.This is done to ensure that the results are reproducible.At this point, this is not strictly necessary, but it is good practice to do so (and when working with Deep Reinforcement Learning, it is absolutely necessary)."
   ],
   "metadata": {
    "id": "hPoctIToiQOV"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "seed = 42\n",
    "rng = np.random.default_rng(seed)"
   ],
   "metadata": {
    "id": "elQOjUDYiehc"
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that we have created our Cliff class, we can instantiate it and show an example of the Grid parameters:\n",
    "* The state index, where each state is represented by an integer number.\n",
    "* The rewards, which as you know, is $-1$ everywhere except on the Cliff region, where we set a very negative reward ($-100$), and on the target state, whose reward is set to $0$ (this is optional, we could have left the $-1$ reward on the final state too).\n",
    "* The final states, which comprise the Cliff states (bad reward) and the target state (zero reward).\n",
    "Take your time comparing the different values, in order to ensure that you understand the problem. Also, you can play around with the dimensions of the Cliff, and see how the representations change."
   ],
   "metadata": {
    "id": "gn5xK_NRfWte"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "env = cliff(dims=(4, 12), seed=seed)\n",
    "\n",
    "# Show reward map\n",
    "n_states = np.prod(env.dims)  # Obtain the number of states in the grid\n",
    "reward_grid = np.zeros(env.dims)  # To store the\n",
    "done_grid = np.zeros(env.dims).astype(bool)  # To store terminal states\n",
    "state_grid = np.zeros(env.dims)  # To store the index of each state\n",
    "\n",
    "for s in range(n_states):  # Check all states\n",
    "  state = env.from_index(s)  # Convert the index to coordinate\n",
    "  r, d = env.reward_done(state)\n",
    "  reward_grid[state[0], state[1]] = r\n",
    "  done_grid[state[0], state[1]] = d\n",
    "  state_grid[state[0], state[1]] = s\n",
    "\n",
    "print('State index in the cliff')\n",
    "print(tabulate(state_grid, tablefmt=\"fancy_grid\"))\n",
    "print('Rewards in the cliff')\n",
    "print(tabulate(reward_grid, tablefmt=\"fancy_grid\"))\n",
    "print('Terminal states in the cliff')\n",
    "print(tabulate(done_grid, tablefmt=\"fancy_grid\"))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wbPQpdo4sXoi",
    "outputId": "fec543ff-4344-4443-9156-cfa44d6a8afe"
   },
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State index in the cliff\n",
      "╒════╤════╤════╤════╤════╤════╤════╤════╤════╤════╤════╤════╕\n",
      "│  0 │  1 │  2 │  3 │  4 │  5 │  6 │  7 │  8 │  9 │ 10 │ 11 │\n",
      "├────┼────┼────┼────┼────┼────┼────┼────┼────┼────┼────┼────┤\n",
      "│ 12 │ 13 │ 14 │ 15 │ 16 │ 17 │ 18 │ 19 │ 20 │ 21 │ 22 │ 23 │\n",
      "├────┼────┼────┼────┼────┼────┼────┼────┼────┼────┼────┼────┤\n",
      "│ 24 │ 25 │ 26 │ 27 │ 28 │ 29 │ 30 │ 31 │ 32 │ 33 │ 34 │ 35 │\n",
      "├────┼────┼────┼────┼────┼────┼────┼────┼────┼────┼────┼────┤\n",
      "│ 36 │ 37 │ 38 │ 39 │ 40 │ 41 │ 42 │ 43 │ 44 │ 45 │ 46 │ 47 │\n",
      "╘════╧════╧════╧════╧════╧════╧════╧════╧════╧════╧════╧════╛\n",
      "Rewards in the cliff\n",
      "╒════╤══════╤══════╤══════╤══════╤══════╤══════╤══════╤══════╤══════╤══════╤════╕\n",
      "│ -1 │   -1 │   -1 │   -1 │   -1 │   -1 │   -1 │   -1 │   -1 │   -1 │   -1 │ -1 │\n",
      "├────┼──────┼──────┼──────┼──────┼──────┼──────┼──────┼──────┼──────┼──────┼────┤\n",
      "│ -1 │   -1 │   -1 │   -1 │   -1 │   -1 │   -1 │   -1 │   -1 │   -1 │   -1 │ -1 │\n",
      "├────┼──────┼──────┼──────┼──────┼──────┼──────┼──────┼──────┼──────┼──────┼────┤\n",
      "│ -1 │   -1 │   -1 │   -1 │   -1 │   -1 │   -1 │   -1 │   -1 │   -1 │   -1 │ -1 │\n",
      "├────┼──────┼──────┼──────┼──────┼──────┼──────┼──────┼──────┼──────┼──────┼────┤\n",
      "│ -1 │ -100 │ -100 │ -100 │ -100 │ -100 │ -100 │ -100 │ -100 │ -100 │ -100 │  0 │\n",
      "╘════╧══════╧══════╧══════╧══════╧══════╧══════╧══════╧══════╧══════╧══════╧════╛\n",
      "Terminal states in the cliff\n",
      "╒═══╤═══╤═══╤═══╤═══╤═══╤═══╤═══╤═══╤═══╤═══╤═══╕\n",
      "│ 0 │ 0 │ 0 │ 0 │ 0 │ 0 │ 0 │ 0 │ 0 │ 0 │ 0 │ 0 │\n",
      "├───┼───┼───┼───┼───┼───┼───┼───┼───┼───┼───┼───┤\n",
      "│ 0 │ 0 │ 0 │ 0 │ 0 │ 0 │ 0 │ 0 │ 0 │ 0 │ 0 │ 0 │\n",
      "├───┼───┼───┼───┼───┼───┼───┼───┼───┼───┼───┼───┤\n",
      "│ 0 │ 0 │ 0 │ 0 │ 0 │ 0 │ 0 │ 0 │ 0 │ 0 │ 0 │ 0 │\n",
      "├───┼───┼───┼───┼───┼───┼───┼───┼───┼───┼───┼───┤\n",
      "│ 0 │ 1 │ 1 │ 1 │ 1 │ 1 │ 1 │ 1 │ 1 │ 1 │ 1 │ 1 │\n",
      "╘═══╧═══╧═══╧═══╧═══╧═══╧═══╧═══╧═══╧═══╧═══╧═══╛\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, we are going to see how to obtain a trajectory by using a random policy. We use a loop that you should become familiar with, as it will be used very frequently in the rest of the course:\n",
    "* First, we obtain the initial state by calling the *reset* method.\n",
    "* Then, we enter a loop which lasts until the *done* flag is active, which means that the episode is over. Note that the loop is needed, as we do not know a priori the episode length.\n",
    "* In each loop iteration, we first select an action, then use that action in the environment by calling the *step* method, and then save all the information for representation.\n",
    "\n",
    "Run the next cell, and observe the output. Also, note that every time that you run the cell, you will obtain a different trajectory, with a different length. You may check that a very tiny proportion of random trajectories do not end in the Cliff, so we need to put more intelligence on the policy - which is what we will do in the rest of the course."
   ],
   "metadata": {
    "id": "N1DJuRcKgjul"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Example of trajectory for random policy\n",
    "states = [env.reset()[0]]  # Obtain initial state with reset() method\n",
    "actions = []\n",
    "rewards = []\n",
    "dones = []\n",
    "done = False  # This controls when to end iterating\n",
    "while not done:\n",
    "  action = env.action_space.sample()  # Randomly sample one of the possible actions\n",
    "  state, reward, terminated, truncated, _ = env.step(action)\n",
    "  done = terminated or truncated\n",
    "  states.append(state)\n",
    "  actions.append(action)\n",
    "  rewards.append(reward)\n",
    "  dones.append(done)\n",
    "\n",
    "# First, show the states indexes\n",
    "print('State index in the grid')\n",
    "print(tabulate(state_grid, tablefmt=\"fancy_grid\"))\n",
    "# Now, show the trajectory values\n",
    "states_index = [env.to_index(s) for s in states]\n",
    "print('States visited in trajectory')\n",
    "print(states_index)\n",
    "print('\\n')\n",
    "print('State / Action / Reward / Done / Next state')\n",
    "pairs = [i for i in zip(states_index, env.action_to_str(actions), rewards, dones, states_index[1:])]\n",
    "for p in pairs:\n",
    "  print(p)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WtE_Gl95uEnJ",
    "outputId": "b376fc81-3145-4e75-d87d-69de803aa947"
   },
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State index in the grid\n",
      "╒════╤════╤════╤════╤════╤════╤════╤════╤════╤════╤════╤════╕\n",
      "│  0 │  1 │  2 │  3 │  4 │  5 │  6 │  7 │  8 │  9 │ 10 │ 11 │\n",
      "├────┼────┼────┼────┼────┼────┼────┼────┼────┼────┼────┼────┤\n",
      "│ 12 │ 13 │ 14 │ 15 │ 16 │ 17 │ 18 │ 19 │ 20 │ 21 │ 22 │ 23 │\n",
      "├────┼────┼────┼────┼────┼────┼────┼────┼────┼────┼────┼────┤\n",
      "│ 24 │ 25 │ 26 │ 27 │ 28 │ 29 │ 30 │ 31 │ 32 │ 33 │ 34 │ 35 │\n",
      "├────┼────┼────┼────┼────┼────┼────┼────┼────┼────┼────┼────┤\n",
      "│ 36 │ 37 │ 38 │ 39 │ 40 │ 41 │ 42 │ 43 │ 44 │ 45 │ 46 │ 47 │\n",
      "╘════╧════╧════╧════╧════╧════╧════╧════╧════╧════╧════╧════╛\n",
      "States visited in trajectory\n",
      "[36, 36, 37]\n",
      "\n",
      "\n",
      "State / Action / Reward / Done / Next state\n",
      "(36, 'down', -1, False, 36)\n",
      "(36, 'right', -100, True, 37)\n"
     ]
    }
   ]
  }
 ]
}
