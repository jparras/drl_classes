{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "**Bellman Optimal Policy**\n",
    "\n",
    "In this exercise, we are going to find the Optimal Policy, using the Bellman equations, for the next MDP:\n",
    "\n",
    "![alt text](two_state_mdp.png \"Title\")\n",
    "\n",
    "Let us start with the imports. We use numpy and scipy."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import fsolve"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, let us define the parameters of the problem. We have an MDP with two states, and two actions. The rewards, discount factor, and transition probabilities are given in the figure above. We also define the four possible deterministic policies as $\\pi_1$, $\\pi_2$, $\\pi_3$, and $\\pi_4$:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "gamma = 0.9\n",
    "R = np.array([[-1, 0.6, 0.5, -0.9]]).T\n",
    "P = np.array([[0.8, 0.2], [0.2, 0.8], [0.3, 0.7], [0.9, 0.1]])\n",
    "pi_1 = np.array([[1, 0, 0, 0], [0, 0, 1, 0]])\n",
    "pi_2 = np.array([[0, 1, 0, 0], [0, 0, 1, 0]])\n",
    "pi_3 = np.array([[1, 0, 0, 0], [0, 0, 0, 1]])\n",
    "pi_4 = np.array([[0, 1, 0, 0], [0, 0, 0, 1]])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, let us implement the Bellman equations, using the fixed point equations seen in the slides.\n",
    "* $v^{\\pi} = \\left( I - \\gamma \\mathcal{P}^{\\pi} \\right)^{-1} \\mathcal{R}^{\\pi}$\n",
    "* $q^{\\pi} = \\left( I - \\gamma \\mathcal{P} \\Pi \\right)^{-1} \\mathcal{R}$\n",
    "Note that, attending to the results, we know that $\\pi_2$ is the optimal policy, as it provides the highest value, as we already know."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy = [1 0 0 0 0 0 1 0]\n",
      "v^pi = [-5.09 -2.36]\n",
      "q^pi = [-5.09 -2.02 -2.36 -5.24]\n",
      "Policy = [0 1 0 0 0 0 1 0]\n",
      "v^pi = [5.34 5.25]\n",
      "q^pi = [3.79 5.34 5.25 3.9 ]\n",
      "Policy = [1 0 0 0 0 0 0 1]\n",
      "v^pi = [-9.83 -9.74]\n",
      "q^pi = [-9.83 -8.19 -8.29 -9.74]\n",
      "Policy = [0 1 0 0 0 0 0 1]\n",
      "v^pi = [-0.63 -1.55]\n",
      "q^pi = [-1.73 -0.63 -0.64 -1.55]\n"
     ]
    }
   ],
   "source": [
    "def bellman_equations(R, P, gamma, pi):\n",
    "    # Code to be filled by the student\n",
    "    return v_pi, q_pi\n",
    "\n",
    "for pi in [pi_1, pi_2, pi_3, pi_4]:\n",
    "    v_pi, q_pi = bellman_equations(R, P, gamma, pi)\n",
    "    with np.printoptions(precision=2, suppress=True):\n",
    "        print(f\"Policy = {pi.flatten()}\")\n",
    "        print(f\"v^pi = {v_pi.flatten()}\")\n",
    "        print(f\"q^pi = {q_pi.flatten()}\")\n",
    "r_opt = bellman_equations(R, P, gamma, pi_2)  # Save the optimal value for later\n",
    "v_opt = r_opt[0].flatten()\n",
    "q_opt = r_opt[1].flatten()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, we are going to obtain the optimal value function using the fsolve method from scipy.optimize. This method finds the root of a function, which is exactly what we need."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal value found v* = [5.34 5.25], optimal value v_opt = [5.34 5.25]\n"
     ]
    }
   ],
   "source": [
    "def bellman_opt_equations(v):\n",
    "    v = v.reshape((2, 1))\n",
    "    Ru = R[0:2]\n",
    "    Rm = R[2:4]\n",
    "    Pu = P[0:2, :]\n",
    "    Pm = P[2:4, :]\n",
    "    aux_term = np.array([np.amax(Ru + gamma * Pu @ v), np.amax(Rm + gamma * Pm @ v)])\n",
    "    return v.flatten() - aux_term\n",
    "v = fsolve(bellman_opt_equations, np.zeros((2, 1)))\n",
    "with np.printoptions(precision=2, suppress=True):\n",
    "    print(f\"Optimal value found v* = {v.flatten()}, optimal value v_opt = {v_opt.flatten()}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, you have to do the same with the state-action value function."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal value found q* = [3.79 5.34 5.25 3.9 ], optimal value q_opt = [3.79 5.34 5.25 3.9 ]\n"
     ]
    }
   ],
   "source": [
    "def bellman_opt_equations(q):\n",
    "    # Code to be filled by the student\n",
    "\n",
    "q = fsolve(bellman_opt_equations, np.zeros((4)))\n",
    "with np.printoptions(precision=2, suppress=True):\n",
    "    print(f\"Optimal value found q* = {q.flatten()}, optimal value q_opt = {q_opt.flatten()}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
