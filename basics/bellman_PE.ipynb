{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "**Bellman Equations for Policy Evaluation**\n",
    "\n",
    "In this exercise, we are going to implement the Bellman equations for policy evaluation in the next MDP:\n",
    "\n",
    "![alt text](two_state_mdp.png \"Title\")\n",
    "\n",
    "Let us start with the imports. We use only numpy in this exercise."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, let us define the parameters of the problem. We have an MDP with two states, and two actions. The rewards, discount factor, and transition probabilities are given in the figure above. We also define the four possible deterministic policies as $\\pi_1$, $\\pi_2$, $\\pi_3$, and $\\pi_4$:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "gamma = 0.9\n",
    "R = np.array([[-1, 0.6, 0.5, -0.9]]).T\n",
    "P = np.array([[0.8, 0.2], [0.2, 0.8], [0.3, 0.7], [0.9, 0.1]])\n",
    "pi_1 = np.array([[1, 0, 0, 0], [0, 0, 1, 0]])\n",
    "pi_2 = np.array([[0, 1, 0, 0], [0, 0, 1, 0]])\n",
    "pi_3 = np.array([[1, 0, 0, 0], [0, 0, 0, 1]])\n",
    "pi_4 = np.array([[0, 1, 0, 0], [0, 0, 0, 1]])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, let us implement the Bellman equations, using the fixed point equations seen in the slides:\n",
    "* $v^{\\pi} = \\left( I - \\gamma \\mathcal{P}^{\\pi} \\right)^{-1} \\mathcal{R}^{\\pi}$\n",
    "* $q^{\\pi} = \\left( I - \\gamma \\mathcal{P} \\Pi \\right)^{-1} \\mathcal{R}$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy = [1 0 0 0 0 0 1 0]\n",
      "v^pi = [-5.09 -2.36]\n",
      "q^pi = [-5.09 -2.02 -2.36 -5.24]\n",
      "Policy = [0 1 0 0 0 0 1 0]\n",
      "v^pi = [5.34 5.25]\n",
      "q^pi = [3.79 5.34 5.25 3.9 ]\n",
      "Policy = [1 0 0 0 0 0 0 1]\n",
      "v^pi = [-9.83 -9.74]\n",
      "q^pi = [-9.83 -8.19 -8.29 -9.74]\n",
      "Policy = [0 1 0 0 0 0 0 1]\n",
      "v^pi = [-0.63 -1.55]\n",
      "q^pi = [-1.73 -0.63 -0.64 -1.55]\n"
     ]
    }
   ],
   "source": [
    "def bellman_equations(R, P, gamma, pi):\n",
    "    # Code to be filled by the student\n",
    "    return v_pi, q_pi\n",
    "\n",
    "for pi in [pi_1, pi_2, pi_3, pi_4]:\n",
    "    v_pi, q_pi = bellman_equations(R, P, gamma, pi)\n",
    "    with np.printoptions(precision=2, suppress=True):\n",
    "        print(f\"Policy = {pi.flatten()}\")\n",
    "        print(f\"v^pi = {v_pi.flatten()}\")\n",
    "        print(f\"q^pi = {q_pi.flatten()}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
