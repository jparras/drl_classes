{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "**Bellman Equations for Policy Evaluation**\n",
    "\n",
    "In this exercise, we are going to implement the Bellman equations for policy evaluation in the next MDP, which is a Random Walk:\n",
    "\n",
    "![alt text](rw.png \"Title\")\n",
    "\n",
    "Let us start with the imports. We use only numpy in this exercise."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, let us define the parameters of the problem. We have an MDP with seven states (two of which are terminal states), and two actions. The rewards, discount factor, and transition probabilities are given in the figure above. For this case, we only evaluate one policy: the uniform random policy, which assigns equal probability to all actions in all states."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "gamma = 0.9\n",
    "R = np.array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]).T\n",
    "P = np.array([[1, 0, 0, 0, 0, 0, 0],\n",
    "              [1, 0, 0, 0, 0, 0, 0],\n",
    "              [0, 0, 1, 0, 0, 0, 0],\n",
    "              [1, 0, 0, 0, 0, 0, 0],\n",
    "              [0, 0, 0, 1, 0, 0, 0],\n",
    "              [0, 1, 0, 0, 0, 0, 0],\n",
    "              [0, 0, 0, 0, 1, 0, 0],\n",
    "              [0, 0, 1, 0, 0, 0, 0],\n",
    "              [0, 0, 0, 0, 0, 1, 0],\n",
    "              [0, 0, 0, 1, 0, 0, 0],\n",
    "              [0, 0, 0, 0, 0, 0, 1],\n",
    "              [0, 0, 0, 0, 1, 0, 0],\n",
    "              [0, 0, 0, 0, 0, 0, 1],\n",
    "              [0, 0, 0, 0, 0, 0, 1]])\n",
    "pi = np.array([[0.5, 0.5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "               [0, 0, 0.5, 0.5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "               [0, 0, 0, 0, 0.5, 0.5, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "               [0, 0, 0, 0, 0, 0, 0.5, 0.5, 0, 0, 0, 0, 0, 0],\n",
    "               [0, 0, 0, 0, 0, 0, 0, 0, 0.5, 0.5, 0, 0, 0, 0],\n",
    "               [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.5, 0.5, 0, 0],\n",
    "               [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.5, 0.5]])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, let us implement the Bellman equations, using the fixed point equations seen in the slides:\n",
    "* $v^{\\pi} = \\left( I - \\gamma \\mathcal{P}^{\\pi} \\right)^{-1} \\mathcal{R}^{\\pi}$\n",
    "* $q^{\\pi} = \\left( I - \\gamma \\mathcal{P} \\Pi \\right)^{-1} \\mathcal{R}$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v^pi = [-0.    0.07  0.15  0.26  0.43  0.69  0.  ]\n",
      "q^pi = [0.   0.   0.13 0.   0.23 0.06 0.38 0.13 0.62 0.23 0.   1.38 0.   0.  ]\n"
     ]
    }
   ],
   "source": [
    "def bellman_equations(R, P, gamma, pi):\n",
    "    # Code to be filled by the student\n",
    "    return v_pi, q_pi\n",
    "\n",
    "v_pi, q_pi = bellman_equations(R, P, gamma, pi)\n",
    "with np.printoptions(precision=2, suppress=True):\n",
    "    print(f\"v^pi = {v_pi.flatten()}\")\n",
    "    print(f\"q^pi = {q_pi.flatten()}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
