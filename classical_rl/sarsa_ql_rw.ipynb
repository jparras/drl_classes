{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "**SARSA and Q-Learning**\n",
    "\n",
    "In this exercise, we are going to compare two algorithms that work model-free for control: SARSA and Q-learning. We will use the next MDP, which is a Random Walk:\n",
    "\n",
    "![alt text](rw.png \"Title\")\n",
    "\n",
    "Let us start with the imports. We use only numpy and matplotlib in this exercise."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The next thing we do is to seed the random number generator.This is done to ensure that the results are reproducible.At this point, this is not strictly necessary, but it is good practice to do so (and when working with Deep Reinforcement Learning, it is absolutely necessary)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(1234)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, let us define the parameters of the problem. We have an MDP with seven states (two of which are terminal states), and two actions. The rewards, discount factor, and transition probabilities are given in the figure above. For this case, we only evaluate one policy: the uniform random policy, which assigns equal probability to all actions in all states."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "gamma = 0.9\n",
    "R = np.array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]).T\n",
    "P = np.array([[1, 0, 0, 0, 0, 0, 0],\n",
    "              [1, 0, 0, 0, 0, 0, 0],\n",
    "              [0, 0, 1, 0, 0, 0, 0],\n",
    "              [1, 0, 0, 0, 0, 0, 0],\n",
    "              [0, 0, 0, 1, 0, 0, 0],\n",
    "              [0, 1, 0, 0, 0, 0, 0],\n",
    "              [0, 0, 0, 0, 1, 0, 0],\n",
    "              [0, 0, 1, 0, 0, 0, 0],\n",
    "              [0, 0, 0, 0, 0, 1, 0],\n",
    "              [0, 0, 0, 1, 0, 0, 0],\n",
    "              [0, 0, 0, 0, 0, 0, 1],\n",
    "              [0, 0, 0, 0, 1, 0, 0],\n",
    "              [0, 0, 0, 0, 0, 0, 1],\n",
    "              [0, 0, 0, 0, 0, 0, 1]])\n",
    "n_states = 7\n",
    "n_actions = 2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, reuse the implementations from the previous exercises to define the methods we are going to use for comparison: Policy Iteration, and Value Iteration. Note that we only use the state-action value function."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "def Policy_Iteration():\n",
    "\n",
    "    # To be filled by the student\n",
    "\n",
    "    return pi_pe  # Return the optimal policy\n",
    "\n",
    "\n",
    "def Value_Iteration():  # No arguments for Value Iteration\n",
    "\n",
    "    # To be filled by the student\n",
    "\n",
    "    return pi_vi  # Return the optimal policy\n",
    "\n",
    "pi_policy_iteration = Policy_Iteration()\n",
    "pi_value_iteration = Value_Iteration()\n",
    "\n",
    "# Obtain the optimal policies for each state: ensure that each policy is a vector of size n_states with the optimal action per state!\n",
    "pi_policy_iteration = np.argmax(pi_policy_iteration[-1], axis=1) % n_actions\n",
    "pi_value_iteration = np.argmax(pi_value_iteration, axis=1) % n_actions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, we are going to implement SARSA and Q-learning algorithms, as it is in your slides. Note that you have to store the value function for each iteration."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining SARSA control...\n",
      "Obtaining Q-Learning control...\n",
      "Policy PI = [0 0 0 0 0 1 0]\n",
      "Policy VI = [0 0 0 0 0 1 0]\n",
      "Policy SARSA = [0 0 0 0 0 1 0]\n",
      "Policy QL = [0 0 0 0 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.02  # Update ratio\n",
    "n_episodes = 500  # Episodes used to learn in SARSA and Q-learning\n",
    "\n",
    "def epsilon_greedy_policy(q, epsilon=0.1):  # Input: q for the given state\n",
    "    q = q.flatten()\n",
    "    if np.random.rand(1) < epsilon:\n",
    "        return np.random.choice(np.arange(q.size))  # Return an action uniformly\n",
    "    else:\n",
    "        return np.argmax(q)  # Action that maximizes q\n",
    "\n",
    "# SARSA\n",
    "print('Obtaining SARSA control...')\n",
    "q_sarsa = np.zeros((n_episodes + 1, n_states, n_actions))\n",
    "for e in range(n_episodes):\n",
    "    # To be filled by the student: Obtain pi_sarsa, the optimal policy for the SARSA algorithm, as a vector of dimension n_states\n",
    "\n",
    "\n",
    "# Q-Learning\n",
    "print('Obtaining Q-Learning control...')\n",
    "q_ql = np.zeros((n_episodes + 1, n_states, n_actions))\n",
    "for e in range(n_episodes):\n",
    "    # To be filled by the student: Obtain pi_ql, the optimal policy for the SARSA algorithm, as a vector of dimension n_states\n",
    "\n",
    "\n",
    "with np.printoptions(precision=2, suppress=True):\n",
    "    print(f\"Policy PI = {pi_policy_iteration.flatten()}\")\n",
    "    print(f\"Policy VI = {pi_value_iteration.flatten()}\")\n",
    "    print(f\"Policy SARSA = {pi_sarsa.astype(int).flatten()}\")\n",
    "    print(f\"Policy QL = {pi_ql.astype(int).flatten()}\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
