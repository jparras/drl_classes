{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "**Linear Approximation limits**\n",
    "\n",
    "In this example, we are going to illustrate the main weakness of Linear Approximations (namely, their dependance with the chosen feature basis), using the next MDP:\n",
    "\n",
    "![alt text](two_state_mdp.png \"Title\")\n",
    "\n",
    "Let us start with the imports. We use numpy and matplotlib in this example, as well as gym and tabulate."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "try:\n",
    "    import gym.spaces as gs\n",
    "except:\n",
    "    !pip install gym\n",
    "    import gym.spaces as gs\n",
    "from tabulate import tabulate"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The next thing we do is to seed the random number generator.This is done to ensure that the results are reproducible.At this point, this is not strictly necessary, but it is good practice to do so (and when working with Deep Reinforcement Learning, it is absolutely necessary)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(1234)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We now define the MDP, and use the Bellman equations to obtain the value function for the random policy (remember that this is an exact solution, and we will use it to compare the results obtained with Linear Approximation)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# Define here the matrices given by the problem\n",
    "n_states = 2\n",
    "n_actions = 2\n",
    "\n",
    "P = np.array([[0.8, 0.2], [0.2, 0.8], [0.3, 0.7], [0.9, 0.1]])\n",
    "R = np.array([[-1], [0.6], [0.5], [-0.9]])\n",
    "gamma = 0.9\n",
    "\n",
    "# Now, define the random policy\n",
    "pi = np.zeros((2, 4))\n",
    "pi[0, 0:2] = 0.5\n",
    "pi[1, 2:] = 0.5\n",
    "\n",
    "# Compute exact values using Bellman matrix equations\n",
    "q_pi_exact = np.linalg.inv(np.eye(P.shape[0]) - gamma * P @ pi) @ R"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In order to solve the MDP, we implement a Gym-like class containing the MDP (this is not strictly necessary, but it is good practice to do so, and it will be useful when we start working with Deep Reinforcement Learning)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "class mdp(object):  # Create a gym-like class for the MDP\n",
    "    def __init__(self, P, R):\n",
    "        self.P = P\n",
    "        self.R = R\n",
    "        self.n_states = self.P.shape[1]\n",
    "        self.n_actions = int(self.P.shape[0] / self.n_states)\n",
    "        self.state_space = gs.Discrete(self.n_states)  # Discrete states\n",
    "        self.action_space = gs.Discrete(self.n_actions)  # Discrete actions\n",
    "        self.state = None\n",
    "        self.t = None  # Counter to have a max number of episode steps\n",
    "        self.max_t = 100  # Max episode steps\n",
    "    def reset(self):\n",
    "        self.state = self.state_space.sample()  # Set the initial state randomly\n",
    "        self.t = 0\n",
    "        return self.state\n",
    "    def step(self, action):\n",
    "        next_state = np.random.choice(np.arange(self.n_states), p=self.P[self.state * self.n_actions + action])\n",
    "        reward = self.R[self.state * self.n_actions + action]\n",
    "        self.t += 1\n",
    "        done = self.t > self.max_t\n",
    "        self.state = next_state\n",
    "        return next_state, reward, done, _"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, define the LSTD algorith, which we use to obtain the value function for the random policy."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def LSTD(features, policy, env, n_episodes, feature_dim, gamma):\n",
    "  gam = np.zeros((feature_dim, feature_dim))\n",
    "  lam = np.zeros((feature_dim, feature_dim))\n",
    "  z = np.zeros((feature_dim, 1))\n",
    "  for e in range(n_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    action = policy(state)\n",
    "    while not done:\n",
    "      next_state, reward, done, _ = env.step(action)\n",
    "      next_action = policy(next_state)\n",
    "      feat = features(state, action)\n",
    "      next_feat = features (next_state, next_action)\n",
    "      gam += feat @ feat.T\n",
    "      lam += feat @ next_feat.T\n",
    "      z += feat * reward\n",
    "      state = next_state\n",
    "      action = next_action\n",
    "  return np.linalg.inv(gam - gamma * lam) @ z  # Linear approximation parameters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, define the random policy and two different set of features to be tested. As the results show, note that the value function obtained with Linear Approximation strongly depends on the chosen feature basis. In this case, One-Hot encoding is a much better choice than using the index of the state-action pair."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State-action value function obtained\n",
      "╒══════════════╤══════════╤═══════════╤═══════════╤═══════════╕\n",
      "│ Method       │   q(x,u) │    q(x,m) │    q(y,u) │    q(y,m) │\n",
      "╞══════════════╪══════════╪═══════════╪═══════════╪═══════════╡\n",
      "│ Exact        │ -2.8     │ -1.2      │ -1.3      │ -2.7      │\n",
      "├──────────────┼──────────┼───────────┼───────────┼───────────┤\n",
      "│ LSTD One-hot │ -2.81434 │ -1.20016  │ -1.30004  │ -2.70129  │\n",
      "├──────────────┼──────────┼───────────┼───────────┼───────────┤\n",
      "│ LSTD Index   │  0       │ -0.148302 │ -0.296605 │ -0.444907 │\n",
      "╘══════════════╧══════════╧═══════════╧═══════════╧═══════════╛\n"
     ]
    }
   ],
   "source": [
    "def policy(state):\n",
    "  return np.random.choice(np.arange(n_actions), p=pi[state, state * n_actions : (state + 1) * n_actions])  # Follows given policy\n",
    "\n",
    "def features_one_hot(state, action):\n",
    "  feature = np.zeros((n_states * n_actions, 1))\n",
    "  feature[state * n_actions + action] = 1\n",
    "  return feature\n",
    "\n",
    "def features_index(state, action):\n",
    "  feature = np.array([[state * n_actions + action]])\n",
    "  return feature\n",
    "\n",
    "# Now, use LSTD\n",
    "\n",
    "env = mdp(P, R)\n",
    "n_episodes = 500\n",
    "\n",
    "q_lstd = []\n",
    "\n",
    "for features in [features_one_hot, features_index]:\n",
    "\n",
    "  feature_dim = features(0, 0).shape[0]\n",
    "\n",
    "  omega = LSTD(features, policy, env, n_episodes, feature_dim, gamma)\n",
    "\n",
    "  # Obtain the value function\n",
    "  feat_mat = np.zeros((n_states * n_actions, feature_dim))\n",
    "  for s in range(n_states):\n",
    "    for a in range(n_actions):\n",
    "      feat_mat[s * n_actions + a] = np.squeeze(features(s, a))\n",
    "\n",
    "  q_lstd.append(feat_mat @ omega)  # The linear approximation is here!\n",
    "\n",
    "values = []\n",
    "values.append(['Exact'])\n",
    "values[-1].extend(list(q_pi_exact))\n",
    "values.append(['LSTD One-hot'])\n",
    "values[-1].extend(list(q_lstd[0]))\n",
    "values.append(['LSTD Index'])\n",
    "values[-1].extend(list(q_lstd[1]))\n",
    "print('State-action value function obtained')\n",
    "print(tabulate(values, tablefmt=\"fancy_grid\", headers=['Method', 'q(x,u)', 'q(x,m)', 'q(y,u)', 'q(y,m)']))"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
