{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "**Linear approximations: feature matrix**\n",
    "\n",
    "In this exercise, we are going to use the following Random Walk to study how linear approximations work for RL. In this part, concretely, we will study how to build the feature matrix that will be key for the linear approximations. The Random Walk is the following:\n",
    "\n",
    "![alt text](rw_2.png \"Title\")\n",
    "\n",
    "Let us start with the imports. We use numpy."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The next thing we do is to seed the random number generator.This is done to ensure that the results are reproducible.At this point, this is not strictly necessary, but it is good practice to do so (and when working with Deep Reinforcement Learning, it is absolutely necessary)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(1234)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The next thing to do is to define the MDP using the data given in the image as:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "n_states = 4\n",
    "n_actions = 2\n",
    "gamma = 0.9  # Discount factor\n",
    "\n",
    "R = np.array([.9, 0.1, .9, 0.1, 0.1, .9, 0.1, .9]).reshape([n_states * n_actions, 1])\n",
    "\n",
    "P = np.array([[.1, .9, 0, 0],\n",
    "              [.9, .1, 0, 0],\n",
    "              [.1, 0, .9, 0],\n",
    "              [.9, 0, .1, 0],\n",
    "              [0, .1, 0, .9],\n",
    "              [0, .9, 0, .1],\n",
    "              [0, 0, .1, .9],\n",
    "              [0, 0, .9, .1]])\n",
    "\n",
    "pi_rp = np.array([[.5, .5, 0, 0, 0, 0, 0, 0],\n",
    "                  [0, 0, .5, .5, 0, 0, 0, 0],\n",
    "                  [0, 0, 0, 0, .5, .5, 0, 0],\n",
    "                  [0, 0, 0, 0, 0, 0, .5, .5]])  # Random policy\n",
    "\n",
    "pi_opt = np.array([[1, 0, 0, 0, 0, 0, 0, 0],\n",
    "                   [0, 0, 1, 0, 0, 0, 0, 0],\n",
    "                   [0, 0, 0, 0, 0, 1, 0, 0],\n",
    "                   [0, 0, 0, 0, 0, 0, 0, 1]])  # Optimal policy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, we are ready to solve the first part of the exercise: finding the optimal value functions (state and state-action) for both given policies: random and optimal. We will use the Bellman equations to do so."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random policy V: [5. 5. 5. 5.]\n",
      "Random policy Q: [5.4 4.6 5.4 4.6 4.6 5.4 4.6 5.4]\n",
      "Optimal policy V: [9. 9. 9. 9.]\n",
      "Optimal policy Q: [9.  8.2 9.  8.2 8.2 9.  8.2 9. ]\n"
     ]
    }
   ],
   "source": [
    "def bellman_equations(pi):\n",
    "\n",
    "    # To be filled by the student\n",
    "\n",
    "    return v_pi, q_pi\n",
    "\n",
    "v_rp, q_rp = bellman_equations(pi_rp)\n",
    "v_opt, q_opt = bellman_equations(pi_opt)\n",
    "\n",
    "with np.printoptions(precision=2, suppress=True):\n",
    "    print(f\"Random policy V: {v_rp.flatten()}\")\n",
    "    print(f\"Random policy Q: {q_rp.flatten()}\")\n",
    "    print(f\"Optimal policy V: {v_opt.flatten()}\")\n",
    "    print(f\"Optimal policy Q: {q_opt.flatten()}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, let us obtain the feature representation for each of the states. We denote the states as $s \\in \\{1, 2, 3, 4\\}$. You have to implement the `get_features` function, which returns the feature representation of a given state."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features for state 1: [0.0967 0.088 ]\n",
      "Features for state 2: [0.0997 0.0967]\n",
      "Features for state 3: [0.0967 0.0997]\n",
      "Features for state 4: [0.088  0.0967]\n"
     ]
    }
   ],
   "source": [
    "n_features = 2\n",
    "def get_features(state, sigma=4):  # Function that returns features for each state\n",
    "\n",
    "    # To be filled by the student\n",
    "\n",
    "    return features\n",
    "\n",
    "for state in range(1, n_states + 1):\n",
    "    with np.printoptions(precision=4, suppress=True):\n",
    "        print(f\"Features for state {state}: {get_features(state)}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Following with the exercise, we now have to obtain each of the feature vectors for the state-action representation."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features for state 1 and action 0: [0.0967 0.088  0.     0.    ]\n",
      "Features for state 2 and action 1: [0.     0.     0.0967 0.088 ]\n",
      "Features for state 1 and action 0: [0.0997 0.0967 0.     0.    ]\n",
      "Features for state 2 and action 1: [0.     0.     0.0997 0.0967]\n",
      "Features for state 1 and action 0: [0.0967 0.0997 0.     0.    ]\n",
      "Features for state 2 and action 1: [0.     0.     0.0967 0.0997]\n",
      "Features for state 1 and action 0: [0.088  0.0967 0.     0.    ]\n",
      "Features for state 2 and action 1: [0.     0.     0.088  0.0967]\n"
     ]
    }
   ],
   "source": [
    "phi = np.zeros((n_states * n_actions, n_features * n_actions))  # Phi matrix, whose components are going to be filled in the next loop\n",
    "for si in range(n_states):\n",
    "    for sa in range(n_actions):\n",
    "        features = np.zeros(n_features * n_actions)  # The phi vector\n",
    "\n",
    "        # To be filled by the student: remember to save the features in the phi matrix\n",
    "\n",
    "        with np.printoptions(precision=4, suppress=True):\n",
    "            print(f\"Features for state {sa + 1} and action {sa}: {features}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The last part of the exercise is inmediate: if everything has been done right, you just obtained the feature matrix needed!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The feature matrix is:\n",
      "[[0.0967 0.088  0.     0.    ]\n",
      " [0.     0.     0.0967 0.088 ]\n",
      " [0.0997 0.0967 0.     0.    ]\n",
      " [0.     0.     0.0997 0.0967]\n",
      " [0.0967 0.0997 0.     0.    ]\n",
      " [0.     0.     0.0967 0.0997]\n",
      " [0.088  0.0967 0.     0.    ]\n",
      " [0.     0.     0.088  0.0967]]\n"
     ]
    }
   ],
   "source": [
    "with np.printoptions(precision=4, suppress=True):\n",
    "    print(\"The feature matrix is:\")\n",
    "    print(phi)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
