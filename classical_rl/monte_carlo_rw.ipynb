{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "**Every-visit Monte-Carlo**\n",
    "\n",
    "In this exercise, we are going to implement the every-visit Monte-Carlo algorithm in the next MDP, which is a Random Walk:\n",
    "\n",
    "![alt text](rw.png \"Title\")\n",
    "\n",
    "Let us start with the imports. We use only numpy in this exercise."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The next thing we do is to seed the random number generator.This is done to ensure that the results are reproducible.At this point, this is not strictly necessary, but it is good practice to do so (and when working with Deep Reinforcement Learning, it is absolutely necessary)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(1234)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, let us define the parameters of the problem. We have an MDP with seven states (two of which are terminal states), and two actions. The rewards, discount factor, and transition probabilities are given in the figure above. For this case, we only evaluate one policy: the uniform random policy, which assigns equal probability to all actions in all states."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "gamma = 0.9\n",
    "R = np.array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]]).T\n",
    "P = np.array([[1, 0, 0, 0, 0, 0, 0],\n",
    "              [1, 0, 0, 0, 0, 0, 0],\n",
    "              [0, 0, 1, 0, 0, 0, 0],\n",
    "              [1, 0, 0, 0, 0, 0, 0],\n",
    "              [0, 0, 0, 1, 0, 0, 0],\n",
    "              [0, 1, 0, 0, 0, 0, 0],\n",
    "              [0, 0, 0, 0, 1, 0, 0],\n",
    "              [0, 0, 1, 0, 0, 0, 0],\n",
    "              [0, 0, 0, 0, 0, 1, 0],\n",
    "              [0, 0, 0, 1, 0, 0, 0],\n",
    "              [0, 0, 0, 0, 0, 0, 1],\n",
    "              [0, 0, 0, 0, 1, 0, 0],\n",
    "              [0, 0, 0, 0, 0, 0, 1],\n",
    "              [0, 0, 0, 0, 0, 0, 1]])\n",
    "pi = np.array([[0.5, 0.5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "               [0, 0, 0.5, 0.5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "               [0, 0, 0, 0, 0.5, 0.5, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "               [0, 0, 0, 0, 0, 0, 0.5, 0.5, 0, 0, 0, 0, 0, 0],\n",
    "               [0, 0, 0, 0, 0, 0, 0, 0, 0.5, 0.5, 0, 0, 0, 0],\n",
    "               [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.5, 0.5, 0, 0],\n",
    "               [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.5, 0.5]])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, let us implement the every-visit version of the Monte-Carlo method to estimate the value function of the policy $\\pi$. You should obtain a similar value function $v^\\pi$ to the one provided."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every-visit Monte-Carlo has converged after 809 iterations\n",
      "v^pi MC = [0.   0.06 0.13 0.23 0.4  0.68 0.  ]\n"
     ]
    }
   ],
   "source": [
    "n_states = 7\n",
    "n_actions = 2\n",
    "n_s = np.zeros((n_states, 1))\n",
    "\n",
    "threshold = 1e-4  # Variation change for convergence check in VI\n",
    "i = 0  # VI iterations\n",
    "delta = 1.0  # Initial difference value\n",
    "v_mc = [np.zeros((n_states, 1))]  # Randomly initiate the value function\n",
    "min_iters = 100  # Set a minimum set of MC iterations: as the reward is sparse, it may (wrongly) converge in the first iterations, if the first episode only has zero rewards\n",
    "\n",
    "converged = False\n",
    "\n",
    "while not converged:\n",
    "    delta = 0\n",
    "    next_state = 3  # Fixed initial state\n",
    "\n",
    "    # To be filled by the student\n",
    "\n",
    "    i += 1\n",
    "    converged = (delta < threshold) and (i > min_iters)\n",
    "\n",
    "print(f\"Every-visit Monte-Carlo has converged after {i} iterations\")\n",
    "\n",
    "with np.printoptions(precision=2, suppress=True):\n",
    "    print(f\"v^pi MC = {v_mc[-1].flatten()}\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
