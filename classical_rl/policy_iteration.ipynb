{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "**Policy Iteration**\n",
    "\n",
    "In this exercise, we are going to implement Policy Iteration, an iterative method to obtain the optimal policy, in the next MDP:\n",
    "\n",
    "![alt text](two_state_mdp.png \"Title\")\n",
    "\n",
    "Let us start with the imports. We use only numpy and matplotlib in this exercise."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, let us define the parameters of the problem. We have an MDP with two states, and two actions. The rewards, discount factor, and transition probabilities are given in the figure above. We also know the optimal policy beforehand from previous exercises:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "gamma = 0.9\n",
    "R = np.array([[-1, 0.6, 0.5, -0.9]]).T\n",
    "P = np.array([[0.8, 0.2], [0.2, 0.8], [0.3, 0.7], [0.9, 0.1]])\n",
    "pi_opt = np.array([[0, 1, 0, 0], [0, 0, 1, 0]])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "First, we obtain the optimal value function $v^{\\pi^* }$ and the optimal action-value function $q^{\\pi^*}$ using the fixed-point Bellman equations, in order to assess the accuracy of our implementation:\n",
    "* $v^{\\pi} = \\left( I - \\gamma \\mathcal{P}^{\\pi} \\right)^{-1} \\mathcal{R}^{\\pi}$\n",
    "* $q^{\\pi} = \\left( I - \\gamma \\mathcal{P} \\Pi \\right)^{-1} \\mathcal{R}$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Policy = [0 1 0 0 0 0 1 0]\n",
      "v^* = [5.34 5.25]\n",
      "q^* = [3.79 5.34 5.25 3.9 ]\n"
     ]
    }
   ],
   "source": [
    "v_opt = (np.linalg.inv(np.eye(pi_opt.shape[0]) - gamma * pi_opt @ P) @ pi_opt @ R).flatten()\n",
    "q_opt = (np.linalg.inv(np.eye(P.shape[0]) - gamma * P @ pi_opt) @ R).flatten()\n",
    "with np.printoptions(precision=2, suppress=True):\n",
    "    print(f\"Optimal Policy = {pi_opt.flatten()}\")\n",
    "    print(f\"v^* = {v_opt}\")\n",
    "    print(f\"q^* = {q_opt}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, we are going to implement Policy Iteration, an iterative method, for the state value function, following the algorithm seen in the slides:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PI converged after  2  iterations\n",
      "Policy optimal theory = [0 1 0 0 0 0 1 0]\n",
      "Policy optimal PI = [0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "v^* theory = [5.34 5.25]\n",
      "v^* PI = [5.33 5.24]\n"
     ]
    }
   ],
   "source": [
    "n_states = 2\n",
    "n_actions = 2\n",
    "pi_pe = [np.zeros((n_states, n_states * n_actions))]  # Initial policy: ensure that it is not the optimal!\n",
    "pi_pe[-1][0,0] = 1\n",
    "pi_pe[-1][1,3] = 1\n",
    "v_pi_pe = [np.zeros((n_states, 1))]  # Initialize the value function to 0\n",
    "\n",
    "threshold = 1e-3  # Variation change for convergence check in PE\n",
    "theta = False  # Indicator of PI convergence\n",
    "i = 0  # PI iterations\n",
    "\n",
    "while not theta:\n",
    "    # First thing: PE loop\n",
    "\n",
    "    # To be filled by the student\n",
    "\n",
    "    # Now, PI loop, to check if the policy is optimal\n",
    "\n",
    "    # To be filled by the student\n",
    "\n",
    "print('PI converged after ', i, ' iterations')\n",
    "\n",
    "\n",
    "with np.printoptions(precision=2, suppress=True):  # Print the values obtained\n",
    "    print(f\"Policy optimal theory = {pi_opt.flatten()}\")\n",
    "    print(f\"Policy optimal PI = {pi_pe[-1].flatten()}\")\n",
    "    print(f\"v^* theory = {v_opt}\")\n",
    "    print(f\"v^* PI = {v_pi_pe[-1].flatten()}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "And we are going to repeat the procedure, but using Policy Iteration on the state-action value function, following the algorithm seen in the slides:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PI converged after  2  iterations\n",
      "Policy optimal theory = [0 1 0 0 0 0 1 0]\n",
      "Policy optimal PI = [0. 1. 0. 0. 0. 0. 1. 0.]\n",
      "q^* theory = [3.79 5.34 5.25 3.9 ]\n",
      "q^* PI = [3.78 5.33 5.24 3.89]\n"
     ]
    }
   ],
   "source": [
    "n_states = 2\n",
    "n_actions = 2\n",
    "pi_pe = [np.zeros((n_states, n_states * n_actions))]  # Initial policy: ensure that it is not the optimal!\n",
    "pi_pe[-1][0,0] = 1\n",
    "pi_pe[-1][1,3] = 1\n",
    "q_pi_pe = [np.zeros((n_states * n_actions, 1))]  # Initialize the value to 0\n",
    "\n",
    "threshold = 1e-3  # Variation change for convergence check in PE\n",
    "theta = False  # Indicator of PI convergence\n",
    "i = 0  # PI iterations\n",
    "\n",
    "while not theta:\n",
    "    # First thing: PE loop\n",
    "\n",
    "    # To be filled by the student\n",
    "\n",
    "    # Now, PI loop, to check if the policy is optimal\n",
    "\n",
    "    # To be filled by the student\n",
    "\n",
    "print('PI converged after ', i, ' iterations')\n",
    "\n",
    "with np.printoptions(precision=2, suppress=True):  # Print the values obtained\n",
    "    print(f\"Policy optimal theory = {pi_opt.flatten()}\")\n",
    "    print(f\"Policy optimal PI = {pi_pe[-1].flatten()}\")\n",
    "    print(f\"q^* theory = {q_opt}\")\n",
    "    print(f\"q^* PI = {q_pi_pe[-1].flatten()}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
