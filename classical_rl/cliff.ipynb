{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "**The Cliff problem**\n",
    "\n",
    "In this exercise, we are going to use the implementations that you should have already done in the previous exercises, in order to solve the Cliff problem:\n",
    "\n",
    "![alt text](cliff_problem.png \"Title\")\n",
    "\n",
    "Let us consider a particular grid-world problem, named the cliff-walking problem, that is appropriate for comparing SARSA and Q-learning. The cliff-walking problem is an episodic task, with one single starting state and one single terminal state, denoted S and G. The action set consists of the usual moving directions for grid-worlds problems: $\\A=\\left\\{ {\\rm up},{\\rm down},{\\rm right},{\\rm left}\\right\\}$ . The reward is $-1$ for all state transitions, except those into the region marked as cliff, such that stepping into this region is penalized with a highly negative reward of $-100$. In addition, transiting to the goal terminal state provides zero reward. When the agent gets to the goal state, the episode ends, and we can start a new episode.\n",
    "\n",
    "Let us start with the imports. We use numpy, as usual, but we also add new libraries that we will use in this exercise: gym, which is a library that contains many environments for RL, and tabulate, which is a library to print tables in a nice way."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "try:\n",
    "    import gym.spaces as gs\n",
    "except:\n",
    "    !pip install gym\n",
    "    import gym.spaces as gs\n",
    "from tabulate import tabulate"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The next thing we do is to seed the random number generator.This is done to ensure that the results are reproducible.At this point, this is not strictly necessary, but it is good practice to do so (and when working with Deep Reinforcement Learning, it is absolutely necessary)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(1234)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The next thing to do is to create the Cliff class, where we are using the Gym environment interface. There are two main methods needed:\n",
    "* `step`: this method implements the environment transition, given an action. It returns the next state, the reward, whether the episode has ended, and some additional information (which we do not use here).\n",
    "* `reset`: this method resets the environment to the initial state. It returns an initial state."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "class cliff(object):  # Create the Cliff class\n",
    "\n",
    "    def __init__(self, dims=(3, 3)):\n",
    "        self.dims = dims  # Gridworld dims (rows x cols)\n",
    "        self.observation_space = gs.Box(low=np.array([0, 0]), high=np.array([self.dims[0] - 1, self.dims[1] - 1]), dtype=int)  # The observations are the coordinates of each grid cell\n",
    "        self.action_space = gs.Discrete(4)  #  There are 4 possible actions: 0=up, 1=down, 2=left, 3=right\n",
    "        self.init_state = (self.dims[0] - 1, 0)  # Initial state, always the same for simplicity: lower left corner\n",
    "        self.target_state = (self.dims[0] - 1, self.dims[1] - 1)  # Target state: always the same for simplicity: lower right corner\n",
    "        self.cliff_location = [(self.dims[0] - 1, i) for i in range(1, self.dims[1] - 1)]  # Lower row (except for target and initial state)\n",
    "        self.state = None  # This is to store data later on\n",
    "\n",
    "    def reset(self, randomize=False):  # Call this method to reset the environment\n",
    "        if randomize:  # To use a random initial state\n",
    "            self.state = self.observation_space.sample()\n",
    "        else:\n",
    "            self.state = self.init_state  # Use a fixed initial state\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):  # This method implements the environment transition\n",
    "\n",
    "        action = np.squeeze(action).astype(int).item()  # Prepare action (make it an integer just in case)\n",
    "        if action < 0 or action > 3:  # Check the action bounds\n",
    "            raise RuntimeError('Action out of bounds')\n",
    "        # Perform action to get next state (i.e., move the agent in the grid world)\n",
    "        if action == 0:\n",
    "            next_state = self.state + np.array([1, 0])  # Move down\n",
    "        elif action == 1:\n",
    "            next_state = self.state + np.array([-1, 0])  # Move up\n",
    "        elif action == 2:\n",
    "            next_state = self.state + np.array([0, -1])  # Move left\n",
    "        elif action == 3:\n",
    "            next_state = self.state + np.array([0, 1])  # Move right\n",
    "        # Set the action bounds correctly, as we may end up out of the grid\n",
    "        next_state = np.clip(next_state, np.zeros(2), np.array(self.dims) - 1).astype(int)\n",
    "        # Check reward and termination condition\n",
    "        reward, done = self.reward_done(next_state)\n",
    "        self.state = next_state  # Change the state in the class\n",
    "        return next_state, reward, done, {}  # Return next state, reward and whether episode has ended\n",
    "\n",
    "    def from_index(self, index):  # Ancillary method: convert an index state to state coordinates\n",
    "        return np.unravel_index(index, self.dims)\n",
    "\n",
    "    def to_index(self, state):  # Ancillary method: convert state coordinates to index\n",
    "        return np.ravel_multi_index(state, self.dims)\n",
    "\n",
    "    def reward_done(self, state):\n",
    "      if tuple(state) in self.cliff_location:  # We fall off the cliff\n",
    "        reward = -100  # Highly penalizing reward: you die\n",
    "        done = True  # Episode is terminated\n",
    "      elif np.sum(np.abs(state - np.array(self.target_state))) < 1e-4:  # Final target found\n",
    "        reward = 0  # Final reward is 0\n",
    "        done = True  # Episode is terminated\n",
    "      else:  # We haven't neither reached the target nor fallen off the cliff\n",
    "        reward = -1  # Standard reward (takes another step)\n",
    "        done = False  # Episode not terminated yet\n",
    "      return reward, done\n",
    "\n",
    "    def action_to_str(self, action):\n",
    "      def ac2str(action):\n",
    "        if action == 0:\n",
    "          return 'down'\n",
    "        if action == 1:\n",
    "          return 'up'\n",
    "        if action == 2:\n",
    "          return 'left'\n",
    "        if action == 3:\n",
    "          return 'right'\n",
    "      if isinstance(action, list):\n",
    "        return [ac2str(a) for a in action]\n",
    "      else:\n",
    "        return ac2str(action)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, we are ready to create the environment and get ready for the RL algorithms:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "env = cliff(dims=(4, 12))  # Environment to work (the cliff)\n",
    "n_states = np.prod(env.dims)  # Number of states, |S|\n",
    "n_actions = 4  # Number of actions, |A|\n",
    "gamma = 0.9"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We are now ready to implement SARSA and Q-learning. Note that you may reuse the code that you have already implemented in a previous exercise, although you may need to adapt to the new environment interface."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining SARSA control...\n",
      "Obtaining Q-Learning control...\n",
      "Policy for SARSA\n",
      "╒═══════╤═══════╤═══════╤═══════╤═══════╤═══════╤═══════╤═══════╤═══════╤═══════╤═══════╤══════╕\n",
      "│ left  │ right │ right │ right │ down  │ right │ right │ right │ right │ right │ right │ down │\n",
      "├───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼──────┤\n",
      "│ right │ right │ right │ right │ right │ right │ right │ right │ right │ right │ right │ down │\n",
      "├───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼──────┤\n",
      "│ up    │ up    │ up    │ up    │ right │ up    │ up    │ up    │ up    │ right │ right │ down │\n",
      "├───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼──────┤\n",
      "│ up    │ down  │ down  │ down  │ down  │ down  │ down  │ down  │ down  │ down  │ down  │ down │\n",
      "╘═══════╧═══════╧═══════╧═══════╧═══════╧═══════╧═══════╧═══════╧═══════╧═══════╧═══════╧══════╛\n",
      "Policy for Q-learning\n",
      "╒═══════╤═══════╤═══════╤═══════╤═══════╤═══════╤═══════╤═══════╤═══════╤═══════╤═══════╤══════╕\n",
      "│ up    │ right │ up    │ right │ right │ right │ right │ right │ down  │ up    │ down  │ down │\n",
      "├───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼──────┤\n",
      "│ right │ down  │ right │ right │ up    │ down  │ right │ right │ down  │ down  │ down  │ down │\n",
      "├───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼──────┤\n",
      "│ right │ right │ right │ right │ right │ right │ right │ right │ right │ right │ right │ down │\n",
      "├───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼──────┤\n",
      "│ up    │ down  │ down  │ down  │ down  │ down  │ down  │ down  │ down  │ down  │ down  │ down │\n",
      "╘═══════╧═══════╧═══════╧═══════╧═══════╧═══════╧═══════╧═══════╧═══════╧═══════╧═══════╧══════╛\n"
     ]
    }
   ],
   "source": [
    "# Implement SARSA and Q-Learning\n",
    "alpha = 0.02  # Update ratio\n",
    "n_episodes = 5000  # Episodes used to update\n",
    "\n",
    "def epsilon_greedy_policy(q, epsilon=0.1):  # Input: q for the given state\n",
    "    if np.random.rand(1) < epsilon:\n",
    "        return np.random.choice(np.arange(q.size))  # Return an action uniformly\n",
    "    else:\n",
    "        return np.argmax(q)  # Action that maximizes q\n",
    "\n",
    "\n",
    "# SARSA\n",
    "print('Obtaining SARSA control...')\n",
    "q_sarsa = np.zeros((n_episodes + 1, n_states, n_actions))\n",
    "for e in range(n_episodes):\n",
    "    # To be filled by the student\n",
    "\n",
    "# Q-Learning\n",
    "print('Obtaining Q-Learning control...')\n",
    "q_ql = np.zeros((n_episodes + 1, n_states, n_actions))\n",
    "for e in range(n_episodes):\n",
    "    # To be filled by the student\n",
    "\n",
    "# For the next part to work, you must provide as output pi_sarsa and pi_ql, which are the policies obtained by SARSA and Q-learning, respectively, as numpy arrays of dimension (n_states, ).\n",
    "\n",
    "pi_sarsa_table = [['' for _ in range(env.dims[1])] for _ in range(env.dims[0])]\n",
    "pi_ql_table = [['' for _ in range(env.dims[1])] for _ in range(env.dims[0])]\n",
    "for s in range(n_states):\n",
    "    state = env.from_index(s)\n",
    "    pi_sarsa_table[state[0]][state[1]] = env.action_to_str(np.argmax(q_sarsa[-1, s, :]))\n",
    "    pi_ql_table[state[0]][state[1]] = env.action_to_str(np.argmax(q_ql[-1, s, :]))\n",
    "\n",
    "print('Policy for SARSA')\n",
    "print(tabulate(pi_sarsa_table, tablefmt=\"fancy_grid\"))\n",
    "print('Policy for Q-learning')\n",
    "print(tabulate(pi_ql_table, tablefmt=\"fancy_grid\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, let us compute the total reward of a trajectory that follows the optimal policy. Note that SARSA reward is lower, as it takes a safer path."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trajectory values: SARSA =  -7.7123207545039 ; Q-learning =  -7.175704635190001\n"
     ]
    }
   ],
   "source": [
    "total_rwd = [0, 0]\n",
    "for i, pi in enumerate([pi_sarsa, pi_ql]):\n",
    "    state = env.to_index(env.reset())\n",
    "    done = False\n",
    "    k = 0\n",
    "    while not done:\n",
    "        action = pi[state]\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        total_rwd[i] += reward * gamma ** k\n",
    "        k += 1\n",
    "        state = env.to_index(next_state)\n",
    "\n",
    "print('Trajectory values: SARSA = ', total_rwd[0], '; Q-learning = ', total_rwd[1])"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
