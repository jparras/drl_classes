{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "**Linear approximations: model-based prediction**\n",
    "\n",
    "In this exercise, we are going to use the following Random Walk to study how linear approximations work for RL. In this part, concretely, we will study how to predict knowing the model. The Random Walk is the following:\n",
    "\n",
    "![alt text](rw_2.png \"Title\")\n",
    "\n",
    "Let us start with the imports. We use numpy."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The next thing we do is to seed the random number generator.This is done to ensure that the results are reproducible.At this point, this is not strictly necessary, but it is good practice to do so (and when working with Deep Reinforcement Learning, it is absolutely necessary)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(1234)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The next thing to do is to define the MDP using the data given in the image as:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "n_states = 4\n",
    "n_actions = 2\n",
    "gamma = 0.9  # Discount factor\n",
    "\n",
    "R = np.array([.9, 0.1, .9, 0.1, 0.1, .9, 0.1, .9]).reshape([n_states * n_actions, 1])\n",
    "\n",
    "P = np.array([[.1, .9, 0, 0],\n",
    "              [.9, .1, 0, 0],\n",
    "              [.1, 0, .9, 0],\n",
    "              [.9, 0, .1, 0],\n",
    "              [0, .1, 0, .9],\n",
    "              [0, .9, 0, .1],\n",
    "              [0, 0, .1, .9],\n",
    "              [0, 0, .9, .1]])\n",
    "\n",
    "pi_rp = np.array([[.5, .5, 0, 0, 0, 0, 0, 0],\n",
    "                  [0, 0, .5, .5, 0, 0, 0, 0],\n",
    "                  [0, 0, 0, 0, .5, .5, 0, 0],\n",
    "                  [0, 0, 0, 0, 0, 0, .5, .5]])  # Random policy\n",
    "\n",
    "pi_opt = np.array([[1, 0, 0, 0, 0, 0, 0, 0],\n",
    "                   [0, 0, 1, 0, 0, 0, 0, 0],\n",
    "                   [0, 0, 0, 0, 0, 1, 0, 0],\n",
    "                   [0, 0, 0, 0, 0, 0, 0, 1]])  # Optimal policy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, we are going to need the feature matrix, that you have computed in a previous exercise. Copy and paste here the code you used to compute it."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The feature matrix is:\n",
      "[[0.0967 0.088  0.     0.    ]\n",
      " [0.     0.     0.0967 0.088 ]\n",
      " [0.0997 0.0967 0.     0.    ]\n",
      " [0.     0.     0.0997 0.0967]\n",
      " [0.0967 0.0997 0.     0.    ]\n",
      " [0.     0.     0.0967 0.0997]\n",
      " [0.088  0.0967 0.     0.    ]\n",
      " [0.     0.     0.088  0.0967]]\n"
     ]
    }
   ],
   "source": [
    "n_features = 2\n",
    "phi = np.zeros((n_states * n_actions, n_features * n_actions))\n",
    "\n",
    "# To be filled by the student\n",
    "\n",
    "with np.printoptions(precision=4, suppress=True):\n",
    "    print(\"The feature matrix is:\")\n",
    "    print(phi)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We now define the other matrices involved in the computation of the BPE. Namely, the visiting probability for each policy."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# Obtain the visiting probability for the random policy\n",
    "P_rp = pi_rp @ P\n",
    "w, v = np.linalg.eig(P_rp.T)\n",
    "eig_unit = np.argmin(np.abs(w - 1))  # Find the unit eigenvector\n",
    "d_v_rp = v[:, eig_unit] / np.sum(v[:, eig_unit])\n",
    "assert np.all(d_v_rp >= 0)\n",
    "d_q_rp = np.zeros([n_states * n_actions])\n",
    "d_q_rp[::2] = d_v_rp\n",
    "d_q_rp[1::2] = d_v_rp\n",
    "d_q_rp = d_q_rp * .5\n",
    "D_q_rp = np.diag(d_q_rp)  # Diagonal matrix with the visiting probability of the random policy\n",
    "\n",
    "# Obtain the visiting probability for the optimal policy\n",
    "P_rp = pi_opt @ P\n",
    "w, v = np.linalg.eig(P_rp.T)\n",
    "eig_unit = np.argmin(np.abs(w - 1))  # Find the unit eigenvector\n",
    "d_v_op = v[:, eig_unit] / np.sum(v[:, eig_unit])\n",
    "assert np.all(d_v_op >= 0)\n",
    "d_q_op = np.zeros([n_states * n_actions])\n",
    "d_q_op[0] = d_v_op[0]\n",
    "d_q_op[2] = d_v_op[1]\n",
    "d_q_op[5] = d_v_op[2]\n",
    "d_q_op[7] = d_v_op[3]\n",
    "D_q_op = np.diag(d_q_op)  # Diagonal matrix with the visiting probability of the optimal policy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You are now ready to apply the BPE equation, as seen in the slides, in order to obtain the approximated state-action value function and the parameters of the linear approximation."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for the random policy\n",
      "Approximated Q-function: [5.41 4.19 5.32 4.89 4.89 5.32 4.19 5.41]\n",
      "Parameters: [ 96.89 -44.9  -44.9   96.89]\n",
      "Results for the optimal policy\n",
      "Approximated Q-function: [9.   7.39 9.   8.43 8.43 9.   7.39 9.  ]\n",
      "Parameters: [137.52 -48.78 -48.78 137.52]\n"
     ]
    }
   ],
   "source": [
    "# To be filled by the student: obtain q_rp_approx, omega_rp, q_op_approx, omega_opt (the Q-value function and the parameters for the random and optimal policy)\n",
    "\n",
    "with np.printoptions(precision=2, suppress=True):\n",
    "    print(\"Results for the random policy\")\n",
    "    print(f\"Approximated Q-function: {q_rp_approx.flatten()}\")\n",
    "    print(f\"Parameters: {omega_rp.flatten()}\")\n",
    "    print(\"Results for the optimal policy\")\n",
    "    print(f\"Approximated Q-function: {q_op_approx.flatten()}\")\n",
    "    print(f\"Parameters: {omega_opt.flatten()}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
