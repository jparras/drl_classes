{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "**Grid World**\n",
    "\n",
    "In this exercise, we are going to use the former implementations that you should have already done in the previous exercises, in order to solve the next Grid World problem:\n",
    "\n",
    "![alt text](grid_world.png \"Title\")\n",
    "\n",
    "Consider a simple grid-world problem, where the environment is a rectangular grid. The state of the environment is given by the location of a robot in the grid. Hence, the cells of the grid correspond to the possible states of the environment. At each cell, four actions are possible: north, south, east and west, which deterministically (i.e., with probability 1) cause the agent to move one cell in the respective direction of the grid. Actions that would take the robot off the grid leave its location unchanged, but also results in a negative (undesired) reward of -1. Other actions result in a reward of 0, except those that move the agent out of the special states A and B. From state A, all four actions yield a positive (desired) reward of +10 and take the agent to A'. From state B, all four actions yield a positive reward of +5 and take the agent to B'.\n",
    "\n",
    "Let us start with the imports. We use only numpy and matplotlib in this exercise."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, let us define the parameters of the problem. We have an MDP with two states, and two actions. The rewards, discount factor, and transition probabilities are given in the figure above: let us explicitly write them (which is tedious)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "gamma = 0.9\n",
    "n_states = 25\n",
    "n_actions = 4\n",
    "\n",
    "def state_action_to_index(s, a): # Ancillary function to convert state-action pairs to indexes\n",
    "    if isinstance(a, str):\n",
    "        a = ['N', 'S', 'E', 'W'].index(a)  # Mapping of actions to indexes\n",
    "    return s * n_actions + a\n",
    "\n",
    "def index_to_state_action(i): # Ancillary function to convert indexes to state-action pairs\n",
    "    return i // n_actions, i % n_actions\n",
    "\n",
    "P = np.zeros((n_states * n_actions, n_states))  # We need to fill the non-zero values, which depend on each state\n",
    "R = np.zeros((n_states * n_actions, 1))  # Already filled with zeros, we only need to add the non-zero values\n",
    "\n",
    "# We fill the non-zero values of P and R case by case:\n",
    "for s in range(n_states):\n",
    "    if s == 0: # Left up corner\n",
    "        P[state_action_to_index(s, 'N'), 0] = P[state_action_to_index(s, 'S'), 5] = P[state_action_to_index(s, 'E'), 1] = P[state_action_to_index(s, 'W'), 0] = 1\n",
    "        R[state_action_to_index(s, 'N')] = R[state_action_to_index(s, 'W')] = -1\n",
    "    elif s == 1:  # Special state A\n",
    "        P[state_action_to_index(s, 'N'), 21] = P[state_action_to_index(s, 'S'), 21] = P[state_action_to_index(s, 'E'), 21] = P[state_action_to_index(s, 'W'), 21] = 1\n",
    "        R[state_action_to_index(s, 'N')] = R[state_action_to_index(s, 'S')] = R[state_action_to_index(s, 'E')] = R[state_action_to_index(s, 'W')] = 10\n",
    "    elif s == 2:\n",
    "        P[state_action_to_index(s, 'N'), 2] = P[state_action_to_index(s, 'S'), 7] = P[state_action_to_index(s, 'E'), 1] = P[state_action_to_index(s, 'W'), 3] = 1\n",
    "        R[state_action_to_index(s, 'N')] = -1\n",
    "    elif s == 3: # Special state B\n",
    "        P[state_action_to_index(s, 'N'), 13] = P[state_action_to_index(s, 'S'), 13] = P[state_action_to_index(s, 'E'), 13] = P[state_action_to_index(s, 'W'), 13] = 1\n",
    "        R[state_action_to_index(s, 'N')] = R[state_action_to_index(s, 'S')] = R[state_action_to_index(s, 'E')] = R[state_action_to_index(s, 'W')] = 5\n",
    "    elif s == 4:  # Right up corner\n",
    "        P[state_action_to_index(s, 'N'), s] = P[state_action_to_index(s, 'S'), s + 5] = P[state_action_to_index(s, 'E'), s] = P[state_action_to_index(s, 'W'), s - 1] = 1\n",
    "        R[state_action_to_index(s, 'N')] = R[state_action_to_index(s, 'E')] = -1\n",
    "    elif s == 20:  # Lower Left corner\n",
    "        P[state_action_to_index(s, 'N'), s - 5] = P[state_action_to_index(s, 'S'), s] = P[state_action_to_index(s, 'E'), s + 1] = P[state_action_to_index(s, 'W'), s] = 1\n",
    "        R[state_action_to_index(s, 'S')] = R[state_action_to_index(s, 'W')] = -1\n",
    "    elif s == 24:  # Lower Right corner\n",
    "        P[state_action_to_index(s, 'N'), s - 5] = P[state_action_to_index(s, 'S'), s] = P[state_action_to_index(s, 'E'), s] = P[state_action_to_index(s, 'W'), s - 1] = 1\n",
    "        R[state_action_to_index(s, 'S')] = R[state_action_to_index(s, 'E')] = -1\n",
    "    elif s == 5 or s == 10 or s == 15:  # Left margin\n",
    "        P[state_action_to_index(s, 'N'), s - 5] = P[state_action_to_index(s, 'S'), s + 5] = P[state_action_to_index(s, 'E'), s + 1] = P[state_action_to_index(s, 'W'), s] = 1\n",
    "        R[state_action_to_index(s, 'W')] = -1\n",
    "    elif s == 9 or s == 14 or s == 19:  # Right margin\n",
    "        P[state_action_to_index(s, 'N'), s - 5] = P[state_action_to_index(s, 'S'), s + 5] = P[state_action_to_index(s, 'E'), s] = P[state_action_to_index(s, 'W'), s - 1] = 1\n",
    "        R[state_action_to_index(s, 'E')] = -1\n",
    "    elif s == 21 or s == 22 or s == 23:  # Down margin\n",
    "        P[state_action_to_index(s, 'N'), s - 5] = P[state_action_to_index(s, 'S'), s] = P[state_action_to_index(s, 'E'), s + 1] = P[state_action_to_index(s, 'W'), s - 1] = 1\n",
    "        R[state_action_to_index(s, 'S')] = -1\n",
    "    else:  # Center cells\n",
    "        P[state_action_to_index(s, 'N'), s - 5] = P[state_action_to_index(s, 'S'), s + 5] = P[state_action_to_index(s, 'E'), s + 1] = P[state_action_to_index(s, 'W'), s - 1] = 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, reuse the implementations from the previous exercises to define the methods we are going to use: Policy Evaluation (using the Bellman equations and the iterative method), Policy Iteration, and Value Iteration. For simplicity, we will only use in thie exercise the state value function (although the same results would be obtained with the state-action value function)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "def Bellman_PE(pi): # pi is the policy to evaluate\n",
    "\n",
    "    # To be filled by the student\n",
    "\n",
    "    return v_opt.flatten()  # Return the value function\n",
    "\n",
    "def Policy_Evaluation(pi):  # pi is the policy to evaluate\n",
    "\n",
    "    # To be filled by the student\n",
    "\n",
    "    return v_pi.flatten()  # Return the value function\n",
    "\n",
    "def Policy_Iteration(pi_init):  # pi init is the initial policy\n",
    "\n",
    "    # To be filled by the student\n",
    "\n",
    "    return pi_pe.flatten(), v_pi_pe.flatten()  # Return the optimal policy and the associated value function\n",
    "\n",
    "def Value_Iteration(): # No arguments for Value Iteration\n",
    "\n",
    "    # To be filled by the student\n",
    "\n",
    "    return pi_vi.flatten(), v_vi.flatten()  # Return the optimal policy and the associated value function"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "First, let us assume a Uniform Random policy: let us obtain the value function for this policy using the Bellman equations and the iterative method:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v^* theory\n",
      "[[ 3.31  8.79  4.43  5.32  1.49]\n",
      " [ 1.52  2.99  2.25  1.91  0.55]\n",
      " [ 0.05  0.74  0.67  0.36 -0.4 ]\n",
      " [-0.97 -0.44 -0.35 -0.59 -1.18]\n",
      " [-1.86 -1.35 -1.23 -1.42 -1.98]]\n",
      "v^* PI\n",
      "[[ 3.32  8.8   4.43  5.33  1.5 ]\n",
      " [ 1.53  3.    2.26  1.91  0.55]\n",
      " [ 0.06  0.74  0.68  0.36 -0.4 ]\n",
      " [-0.97 -0.43 -0.35 -0.58 -1.18]\n",
      " [-1.85 -1.34 -1.22 -1.42 -1.97]]\n"
     ]
    }
   ],
   "source": [
    "pi_random = np.zeros((n_states, n_states * n_actions))\n",
    "for s in range(n_states):\n",
    "    pi_random[s, s * n_actions: (s + 1) * n_actions] = 1 / n_actions  # This is a uniform random policy\n",
    "\n",
    "v_bellman = Bellman_PE(pi_random)\n",
    "v_pe = Policy_Evaluation(pi_random)\n",
    "\n",
    "with np.printoptions(precision=2, suppress=True):  # Print the values obtained\n",
    "    print(\"v^* theory\")\n",
    "    print(v_bellman.reshape((5, 5)))\n",
    "    print(\"v^* PI\")\n",
    "    print(v_pe.reshape((5, 5)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Second, we are going to obtain the optimal policy and the associated value function using both Policy Iteration and Value Iteration:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v^* PI\n",
      "[[21.98 24.42 21.98 19.42 17.48]\n",
      " [19.78 21.98 19.78 17.8  16.02]\n",
      " [17.8  19.78 17.8  16.02 14.42]\n",
      " [16.02 17.8  16.02 14.42 12.98]\n",
      " [14.42 16.02 14.42 12.98 11.68]]\n",
      "v^* VI\n",
      "[[21.98 24.42 21.98 19.42 17.48]\n",
      " [19.78 21.98 19.78 17.8  16.02]\n",
      " [17.8  19.78 17.8  16.02 14.42]\n",
      " [16.02 17.8  16.02 14.42 12.98]\n",
      " [14.42 16.02 14.42 12.98 11.68]]\n"
     ]
    }
   ],
   "source": [
    "pi_opt, v_opt = Policy_Iteration(pi_random)\n",
    "pi_vi, v_vi = Value_Iteration()\n",
    "\n",
    "with np.printoptions(precision=2, suppress=True):  # Print the values obtained\n",
    "    print(\"v^* PI\")\n",
    "    print(v_opt.reshape((5, 5)))\n",
    "    print(\"v^* VI\")\n",
    "    print(v_vi.reshape((5, 5)))"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
