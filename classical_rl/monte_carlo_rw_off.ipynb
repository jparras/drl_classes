{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "**Off-policy Monte-Carlo via Importance Sampling**\n",
    "\n",
    "In this exercise, we are going to implement the off-policy Monte-Carlo algorithm using Importance Sampling in the next MDP, which is a Random Walk:\n",
    "\n",
    "![alt text](rw.png \"Title\")\n",
    "\n",
    "Let us start with the imports. We use only numpy in this exercise."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The next thing we do is to seed the random number generator.This is done to ensure that the results are reproducible.At this point, this is not strictly necessary, but it is good practice to do so (and when working with Deep Reinforcement Learning, it is absolutely necessary)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(1234)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, let us define the parameters of the problem. We have an MDP with seven states (two of which are terminal states), and two actions. The rewards, discount factor, and transition probabilities are given in the figure above."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "gamma = 0.9\n",
    "R = np.array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]]).T\n",
    "P = np.array([[1, 0, 0, 0, 0, 0, 0],\n",
    "              [1, 0, 0, 0, 0, 0, 0],\n",
    "              [0, 0, 1, 0, 0, 0, 0],\n",
    "              [1, 0, 0, 0, 0, 0, 0],\n",
    "              [0, 0, 0, 1, 0, 0, 0],\n",
    "              [0, 1, 0, 0, 0, 0, 0],\n",
    "              [0, 0, 0, 0, 1, 0, 0],\n",
    "              [0, 0, 1, 0, 0, 0, 0],\n",
    "              [0, 0, 0, 0, 0, 1, 0],\n",
    "              [0, 0, 0, 1, 0, 0, 0],\n",
    "              [0, 0, 0, 0, 0, 0, 1],\n",
    "              [0, 0, 0, 0, 1, 0, 0],\n",
    "              [0, 0, 0, 0, 0, 0, 1],\n",
    "              [0, 0, 0, 0, 0, 0, 1]])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Our problem now is to estimate the value function of the policy $\\pi$, which assigns probability $2/3$ to the right action, and $1/3$ for the left action, when we have samples following the uniform random policy $\\mu$. Before anything, we will compute the theorical value function $v^\\pi$ with the Bellman equation."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v^pi = [0.     0.2291 0.3818 0.5217 0.6787 0.8703 0.    ]\n",
      "v^pi(4) = 0.5217391304347825\n"
     ]
    }
   ],
   "source": [
    "pi = np.array([[2/3, 1/3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "               [0, 0, 2/3, 1/3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "               [0, 0, 0, 0, 2/3, 1/3, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "               [0, 0, 0, 0, 0, 0, 2/3, 1/3, 0, 0, 0, 0, 0, 0],\n",
    "               [0, 0, 0, 0, 0, 0, 0, 0, 2/3, 1/3, 0, 0, 0, 0],\n",
    "               [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2/3, 1/3, 0, 0],\n",
    "               [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2/3, 1/3]])\n",
    "\n",
    "v_pi = np.linalg.inv(np.eye(pi.shape[0]) - gamma * pi @ P) @ pi @ R\n",
    "\n",
    "with np.printoptions(precision=4, suppress=True):\n",
    "    print(f\"v^pi = {v_pi.flatten()}\")\n",
    "    print(f\"v^pi(4) = {v_pi[3, 0]}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, let us implement the off-policy Monte-Carlo method with importance sampling . You should obtain a similar value function $v^\\pi$ to the one provided."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value estimated for state 4 = 0.542847170693135\n",
      "Theorical value for state 4 = 0.5217391304347825\n"
     ]
    }
   ],
   "source": [
    "n_states = 7\n",
    "n_actions = 2\n",
    "\n",
    "threshold = 1e-4  # Variation change for convergence check\n",
    "delta = 1.0  # Initial difference value\n",
    "v_s4 = []  # List of value functions estimated for state 4\n",
    "iters = 2000  # Number of iterations\n",
    "\n",
    "pi_probs = np.array([2/3, 1/3])  # Probabilities of the policy pi\n",
    "\n",
    "for i in range(iters):\n",
    "\n",
    "    # To be filled by the student: append to v_s4 the estimation of the value function for state 4 using Importance Sampling\n",
    "\n",
    "with np.printoptions(precision=4, suppress=True):\n",
    "    print(f\"Value estimated for state 4 = {np.mean(v_s4)}\")\n",
    "    print(f\"Theorical value for state 4 = {v_pi[3, 0]}\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
